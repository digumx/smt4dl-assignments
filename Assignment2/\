"""
Script to train a network using data generated by datagen and using tensorflow.keras
"""

import os.path

from tensorflow.keras import models, layers, losses
import z3

from datagen import *
from encode_dnn import *
from game_props import *



"""
Generate Data
"""

data = []
num = 10000

# Check if datafile exists
if os.path.isfile("./out/data.val"):
    with open("./out/data.val") as f:
        data = eval(f.read())
        num = len(data)
else:
    with open("./out/data.val", "w") as f:
        data = gen_data(num)
        f.write(str(data))




"""
Make and train DNN
"""

# Create a simple DNN model
dnn = models.Sequential([
        layers.Input(shape = (36,)),
        layers.Dense(20, activation='relu'),
        layers.Dense(10, activation='relu'),
        layers.Dense(2, activation='relu'),])

# We do not use the softmax normalization in the DNN itself. Instead, we bake a normalization into
# the loss function by using the BinaryCrossentropy function with the from_logits flag set to True.
# This will effectively apply softmax while calculating loss, but the model itself does not apply
# softmax to the output.
dnn.compile(loss = losses.BinaryCrossentropy(from_logits = True), metrics = ['accuracy'])

# Split training and testing data, prepare data
train_x = list(map(lambda x : [1 if c else 0 for c in x], [p[0] for p in data[:int(0.6*num)]]))
train_y = [([0, 1] if p[1] else [1, 0]) for p in data[:int(0.6*num)]]
test_x = list(map(lambda x : [1 if c else 0 for c in x], [p[0] for p in data[int(0.6*num):]]))
test_y = [([0, 1] if p[1] else [1, 0]) for p in data[int(0.6*num):]]

# Train model
dnn.fit(train_x, train_y, epochs=10) #80)

# Test model
dnn.evaluate(test_x, test_y, verbose=2)

# Get weights
#print([(type(l.get_weights()), l.get_weights()) for l in dnn.layers])



"""
Verify properties
"""

# We first create a solver and constants
solver = z3.Solver()
z3_move =   [z3.Const("move_"+str(i), z3.BoolSort()) for i in range(36)]        # Move
z3_res =    [z3.Const("res_"+str(i), z3.BoolSort()) for i in range(27)]         # Result of move
z3_inp =    [z3.Const("inp_"+str(i), z3.RealSort()) for i in range(36)]         # Input to nn

# We assert that z3_move is valid
assert_valid_move(solver, z3_move)
# z3_res is the result of doing the move
solver.add(encode_move(z3_move[:27], z3_res, z3_move[27:], 1))
# z3_inp encodes input for nn
assert_input_encoding(solver, z3_move, z3_inp)

# We get the weights and biases of the network
weights = [l.get_weights()[0].tolist() for l in dnn.layers]
biases =  [l.get_weights()[1].tolist() for l in dnn.layers]

# The expression encoding the output of the network
z3_out = encode_network(weights, biases, z3_inp)

# Assert that the network classifies output as good
solver.add(z3_out[1] > z3_out[0])
# But player 2 can win from resulting config
solver.add(encode_has_winning_move(z3_move[:27], 2))

print("Check for Q3: " + ("Failed" if solver.check() == z3.sat else "Passed"))
