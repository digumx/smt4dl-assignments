# -*- coding: utf-8 -*-
"""Network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gs0amwvCFsfHmKcG8MpDfrUTJ0YGeqRB

# Introduction to Neural Network

---



We will implement a basic ANN class
"""

import numpy as np
import matplotlib.pyplot as plt
import random
import math

"""### Few helper functions for the Neural Network"""

def relu(x):
    # input is numpy vector
    return np.maximum(x,0)


vfunc = np.vectorize(lambda x : 1 if x > 0 else 0,  otypes=[np.float])  ## vectorizing allows us to
                                                                        ## apply arbitrary function to all the element of a numpy vector
def relu_derivative(x):
    # input is numpy vector
    return vfunc(x)
    #return (x > 0).astype(int)  # this replaces all the positive value by 1 and rest by 0

def sigmoid(x):
    # input is numpy vector
    return 1.0/(1.0+np.exp(-x))

def sigmoid_derivative(x):
    return sigmoid(x) * (1-sigmoid(x))

def mean_square_derivative(output_activations, y):
    return (output_activations-y)

# following function will be used to transfor the training lables for digits recognition
def vectorized_result(j): 
    # Returns 10-size column with 1 at jth position and rest are 0
    e = np.zeros((10, 1))
    e[j] = 1.0
    return e

"""# Network Class
#### Finish the code of the Network Class. You are free to add/remove attributes as required. Make
sure that the signature of **\_\_init\_\_()** and **SGD** methods are not changed as they will be
called in the later part of the notebook

---
After writing the class, run the rest of the code and verify it works properly
"""

class Network:
    def __init__(self, sizes, activation_func = sigmoid, activation_derivative = sigmoid_derivative ):
        """
        Objective: Intialize the network
        Input:sizes - tuple representing number of neurons in each layer from left to right
              Note: sizes[0] is the number of inputs: they are not neurons per se
              E.g., sizes = (5,4,2) means that there are 5 inputs, 4 neurons in the first hidden layer, and 2 neuron in the ouput layer
        """
        self.train_cost_history= []    ## Can be used to store costs w.r.t training data w.r.t iteration number
        self.test_cost_history = []    ## Can be used to store costs w.r.t testing data w.r.t iteration number

        self.activation = activation_func
        self.activation_derivative = activation_derivative
        self.cost_derivative = mean_square_derivative
        
        self.num_layers = len(sizes)   ### Number of layers is length of list sizes
        
        self.biases = [np.random.randn(number_of_neurons, 1) for number_of_neurons in sizes[1:]]
        
        self.weights = [np.random.randn(y, x)
                        for x, y in zip(sizes[:-1], sizes[1:])] 
   

    def feedforward(self, x):
        """
        Return the output of the network for the input 'x'.
        """
        for b, w in zip(self.biases, self.weights):
            x = self.activation(np.dot(w, x)+b)
        return x
   

    def SGD(self, training_data, epochs, mini_batch_size, learning_rate, test_data=None):
        """
        Objective: Stochastic Gradient Descent
        1. tranining_data is a python list of tuples of the form (x, y) 
        where x is the input vector (numpy matrix of size (n x 1)) and y is the output label
        2. epochs is the number of epochs
        3. mini_batch_size is the size of single mini-batch on which we will perform the backpropogation
        4. learning_rate is learning rate!
        5. test_data is optional in the same format as training data: can be used to calculate accuracy on each
        iteration. (Ignore test_data while implementing for first time)
        """
        num_batches = len(training_data) // mini_batch_size                     # This must be int
        for it in range(epochs):
            print("Epoch ", it+1, "/", epochs)

            # Shuffle training data
            random.shuffle(training_data)

            # Split into batches
            batches = (training_data[i:i+mini_batch_size] for i in 
                            range(0, len(training_data), mini_batch_size))
            
            # Backprop on each batch
            for batch,batch_num in zip(batches, range(1, num_batches+1)):
                print("Batch ", batch_num, "/", num_batches, end='\r')
                self.apply_backprop_on_batch(batch, learning_rate)
            print()

            # Calculate training and testing costs and add to history
            if(test_data != None):
                cost_train = self.test_accuracy(training_data)
                print("Training cost: ", cost_train) 
                self.train_cost_history.append(cost_train) 
                cost_test = self.test_accuracy(test_data)
                print("Testing cost: ", cost_test) 
                self.test_cost_history.append(cost_test) 
        
    
    def apply_backprop_on_batch(self, batch, learning_rate):
        """
        Applies the backprop on the given batch and updates weights and biases
        
        """
        gradient_biases = [np.zeros(b.shape) for b in self.biases]
        gradient_weights = [np.zeros(w.shape) for w in self.weights]
        
        for x,y in batch:
            # x, y is one training example
            gradient_biases_on_example, gradient_weights_on_example = self.apply_backprop_on_example(x,y)
            gradient_biases = [db + dbi for db,dbi in                                       # Sum up 
                                zip(gradient_biases, gradient_biases_on_example)]
            gradient_weights = [dw + dwi for dw,dwi in 
                                zip(gradient_weights, gradient_weights_on_example)]
        map(lambda m: m * (learning_rate/len(batch)), gradient_biases)                       # Divide
        map(lambda m: m * (learning_rate/len(batch)), gradient_weights)                      # and
                                                                                             # Normalize
        # Update weights and biases
        self.weights = [w - dw for w,dw in zip(self.weights, gradient_weights)]
        self.biases = [b - db for b,db in zip(self.biases, gradient_biases)]
   

    def apply_backprop_on_example(self, x, y ):
        """
        Applies backpropagation and calculates partial derivates for all the weights and biases
        for one given example (x,y)
        NOTE: It does NOT updates weights/biases. That is done by the caller function
        """
        # First we perform a feed-forward to get the values of the inputs to each layer. We repeat
        # the code here because it also appends to a list, an operation which is unnecessary in the
        # general feed forward. We also store the activation function derivatives.
        layer_inputs = []
        output_activations = x
        activation_function_derivs = []
        for w, b in zip(self.weights, self.biases):
            layer_inputs.insert(0, output_activations)
            tmp = np.dot(w, output_activations) + b
            output_activations = self.activation(tmp)
            activation_function_derivs.insert(0, self.activation_derivative(tmp))


        # Now we do backprop
        biases_gradients = []
        weights_gradients = []
        # The following variable collects the term in the chain rule expansion of dC/dw for the
        # layers before which w occurs in, chained with the derivative of the norm square.
        chain_pre_term = np.transpose(mean_square_derivative(output_activations, y))

        for w,b,i,da in zip(reversed(self.weights), 
                            reversed(self.biases), 
                            layer_inputs,
                            activation_function_derivs):
            #print("DEBUG: ", output_activations, y, chain_pre_term, self.activation_derivative)
            chain_pre_term *= np.transpose(da)                                  # Chain sparse
                                                                                # activation deriv
            biases_gradients.insert(0, np.transpose(chain_pre_term))            # d(Wx + b)/db = 1
            weights_gradients.insert(0, np.outer(chain_pre_term, i))            # Linalg checks out
            #print("DEBUG: ", w.shape, chain_pre_term.shape, da.shape)
            chain_pre_term = np.dot(chain_pre_term, w)                          # Chain d(Wx+b)/dx
                                                                                #       = W

        return biases_gradients, weights_gradients
    
    def test_accuracy(self, test_data):
        err = 0
        for (x,y),i in zip(test_data, range(1, len(test_data)+1)):
            print("Testing accuracy: ", i, "/", len(test_data), end='\r')
            out = self.feedforward(x)
            err += np.linalg.norm(out-y)**2
        print()
        return err / len(test_data)


# """# Digit recognition using Network
# 
# ---
# We import the data from mnist.
# """
# 
# # TO THE TA: On my system, I could not get keras itself installed in an easy manner. The github page for
# # Keras (https://github.com/keras-team/keras) says that Multi-Backend Keras is being deprecated and
# # suggests switching to tensorflow.keras. Since we are only using it for datasets, and I do have
# # tensorflow installed, I changed the import in the next line to use tensorflow.keras
# from tensorflow.keras.datasets import mnist  # NOTE: keras is only used to import data
# 
# (x_train, y_train), (x_test, y_test) = mnist.load_data()
# y_train_vectorized = list(map(vectorized_result, y_train))   # This will convert digit 0 to [1.0,0,0,0..0], 1 to [0,1.0,0,0...0] and so on...
# x_train_vectorized = list(map(lambda x: np.reshape(x, (784,1))/255, x_train)) 
# # print(y_train_vectorized[0])
# 
# training_data = list(zip(x_train_vectorized, y_train_vectorized))
# # print(training_data[0])
# 
# x_test_vectorized = list(map(lambda x: np.reshape(x, (784,1))/255, x_test)) 
# test_data = list(zip(x_test_vectorized, y_test))   # Notice carefully that we have not vectorized the y_test
# 
# # TO THE TA:
# # We will need to vectorize y_test for the test_accuracy function to work, right? So if we are going
# # to pass test data to `SGC()`, it must be vectorized right?
# vectorized_test_data = list(zip(x_test_vectorized, map(vectorized_result, y_test)))
# 
# net = Network([784, 16, 10])  # activation_func = relu, activation_derivative = relu_derivative)
# n_epochs = 100 #30
# net.SGD(training_data, n_epochs, 10, 0.1, test_data=vectorized_test_data)   # using training_data[:20000] just to make epochs faster. Consider using full training data
# #net.SGD(training_data[:10000], n_epochs, 10, 0.1, test_data=training_data[:10000])   # using training_data[:20000] just to make epochs faster. Consider using full training data
# 
# i = 99
# plt.imshow(x_test[i],  cmap='gray')
# print("Label is:", y_test[i])
# print("Network's Prediction:", np.argmax(net.feedforward(x_test_vectorized[i])))
# plt.show()                              # TO THE TA: I need this on my system for the image to appear
# 
# # We print out final accuracy
# n = 0
# for x,y in zip(x_test_vectorized, y_test):
#     n += 1 if y == np.argmax(net.feedforward(x)) else 0
# 
# print("Percent accuracy", 100 * n /len(y_test))
# 
# 
# """
# ---
# The code prints three waves on a graph. Of course, you need to change it to plot training cost and
# testing cost w.r.t iteration number.
# """
# 
# plt.plot(range(n_epochs), net.train_cost_history, 'g',  label = "Training")
# plt.plot(range(n_epochs), net.test_cost_history, 'r',  label = "Testing")
# plt.legend()
# plt.xlabel("Epochs")
# plt.ylabel("Mean Square Cost")
# plt.title("Training vs Testing cost over Epochs")
# plt.grid()
# plt.show()
# 
# 
# 
"""# NN Example \#1 : Shapes"""

import drawSvg as draw

def drawRect(d, x, y, color):
    d.append(draw.Rectangle(x,y,9,9, fill=color,))
    d.setPixelScale(2)  # Set number of pixels per geometry unit
    # Display in Jupyter notebook
    d.rasterize()  # Display as PNG
    return d  # Display as SVG

def rgb2hex(r,g,b):
    return "#{:02x}{:02x}{:02x}".format(r,g,b)

def flattenMatrix(shape):
    out = list()
    for row in shape:
        out.extend(row)
    return out


def drawMatrix(mat, color = None):
    # values expected b/w 0 and 1
    if color == None:
        r,g,b = 255,0,0
    else:
        r,g,b = color
    m = len(mat)
    n = len(mat[0])
    d = draw.Drawing(n*10, m*10, displayInline=False)
    for row in range(0,m):
        for col in range(0,n):
            x,y= col*10, (m-1-row)*10
            active = mat[row][col]
            color = rgb2hex(round(r*active), round(g*active), round(b*active))
            drawRect(d, x, y, color)
    return d

cap = [[0.9,0.9,0.9,0.9],[0.9,0,0,0.9],[0.9,0,0,0.9],[0,0,0,0]]
L_shape = [[0.9,0,0,0],[0.9,0,0,0],[0.9,0,0,0],[0.9,0.9,0.9,0.9]]
square = [[0.9,0.9,0.9,0.9],[0.9,0,0,0.9],[0.9,0,0,0.9],[0.9,0.9,0.9,0.9]]

mat = np.random.randint(low = 0, high = 2, size = (4,4))
drawMatrix(cap).rasterize()

drawMatrix(L_shape).rasterize()

"""# Network to detect the shapes : L-Shape, Square, Cap"""

shape_net = Network([16,6,4])   # 16 is number of inputs, 4 is the number of outputs
flatten_cap = np.array(cap).reshape((16,1))
out = shape_net.feedforward(flatten_cap)
result = np.argmax(out)
print(out)

# Create dataset for training
def getNoiseMatrix(mat):
    return np.array(mat) + np.random.normal(0, .05, np.array(mat).shape)
drawMatrix(getNoiseMatrix(square).tolist()).rasterize()

training_shapes = []
training_shapes_labels = []  ## Label would be [Square, Cap, L, other] :: [1,0,0]
## Test label mapping:  Square = 0, Cap = 1, L = 2, other = 3

def createShapeData(size):
    """
    This function is used to create artificial data for training shape detector
    """
    shapes = []
    labels = []
    for i in range(size//4):
        noised_square = getNoiseMatrix(square).reshape((16,1))
        noised_cap = getNoiseMatrix(cap).reshape((16,1))
        noised_L = getNoiseMatrix(L_shape).reshape((16,1))
        other =  np.random.normal(.5, .5, (16,1))
        
        shapes.extend([noised_square, noised_cap, noised_L, other])
        labels.extend(map(convert_to_np_label, [[1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]]))
    return shapes, labels

def convert_to_np_label(lst):
    return np.array(lst).reshape(4,1)
        
training_shapes, training_labels = createShapeData(1000) 
test_shapes, test_labels = createShapeData(200) 
# need to convert test_labels to numbers
test_labels = [np.argmax(i) for i in test_labels]
# zip the data for Network
s_training_data = list(zip(training_shapes, training_labels))
s_test_data = list(zip(test_shapes, test_labels))

print(training_labels[1])
drawMatrix(training_shapes[2].reshape(4,4).tolist()).rasterize()

"""## Train the shape_net with the generated data"""

shape_net.SGD(s_training_data, 30, 10, 0.1, s_test_data)
label_map = ["Square", "Cap", "L-Shape", "Other"]

# helper function to apply the model to get the shape name
def getShape(mat):
    out = shape_net.feedforward(mat.reshape(16,1))
    result = np.argmax(out)
    print(result)
    return label_map[result]

"""## Testing the model manually"""

test_mat = np.array([
    0.9,0.88, 0.7,  0.9,
    0.9,0.22,0.1,  0.9,
    0.9,0.21,0.02, 0.9,
    0.9,0.8, 0.9,  0.9]
).reshape((4,4))

print("Predicted Shape is: ", getShape(test_mat))
drawMatrix(test_mat.tolist())

test_mat = np.array([
    0.9,0.2, 0.9,  0.0,
    0.9,0.22,0.1,  0.01,
    0.9,0.21,0.02, 0.02,
    0.9,0.8, 0.9,  0.01]
).reshape((4,4))

print("Predicted Shape is: ", getShape(test_mat))
drawMatrix(test_mat.tolist())

#@title
# we can see the weights of the neurons using following
print(shape_net.weights[0])
