# -*- coding: utf-8 -*-
"""Network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gs0amwvCFsfHmKcG8MpDfrUTJ0YGeqRB

# Introduction to Neural Network

---



We will implement a basic ANN class
"""

import numpy as np
import matplotlib.pyplot as plt
import random
import math

"""### Few helper functions for the Neural Network"""

def relu(x):
    # input is numpy vector
    return np.maximum(x,0)


vfunc = np.vectorize(lambda x : 1 if x > 0 else 0,  otypes=[np.float])  ## vectorizing allows us to
                                                                        ## apply arbitrary function to all the element of a numpy vector
def relu_derivative(x):
    # input is numpy vector
    return vfunc(x)
    #return (x > 0).astype(int)  # this replaces all the positive value by 1 and rest by 0

def sigmoid(x):
    # input is numpy vector
    return 1.0/(1.0+np.exp(-x))

def sigmoid_derivative(x):
    return sigmoid(x) * (1-sigmoid(x))

def mean_square_derivative(output_activations, y):
    return (output_activations-y)

# following function will be used to transfor the training lables for digits recognition
def vectorized_result(j): 
    # Returns 10-size column with 1 at jth position and rest are 0
    e = np.zeros((10, 1))
    e[j] = 1.0
    return e

"""# Network Class
#### Finish the code of the Network Class. You are free to add/remove attributes as required. Make
sure that the signature of **\_\_init\_\_()** and **SGD** methods are not changed as they will be
called in the later part of the notebook

---
After writing the class, run the rest of the code and verify it works properly
"""

class Network:
    def __init__(self, sizes, activation_func = sigmoid, activation_derivative = sigmoid_derivative ):
        """
        Objective: Intialize the network
        Input:sizes - tuple representing number of neurons in each layer from left to right
              Note: sizes[0] is the number of inputs: they are not neurons per se
              E.g., sizes = (5,4,2) means that there are 5 inputs, 4 neurons in the first hidden layer, and 2 neuron in the ouput layer
        """
        self.train_cost_history= []    ## Can be used to store costs w.r.t training data w.r.t iteration number
        self.test_cost_history = []    ## Can be used to store costs w.r.t testing data w.r.t iteration number

        self.activation = activation_func
        self.activation_derivative = activation_derivative
        self.cost_derivative = mean_square_derivative
        
        self.num_layers = len(sizes)   ### Number of layers is length of list sizes
        
        self.biases = [np.random.randn(number_of_neurons, 1) for number_of_neurons in sizes[1:]]
        
        self.weights = [np.random.randn(y, x)
                        for x, y in zip(sizes[:-1], sizes[1:])] 
        
    def feedforward(self, x):
        """
        Return the output of the network for the input 'x'.
        """
        for b, w in zip(self.biases, self.weights):
            x = self.activation_func(np.dot(w, x)+b)
        return x
    
    def SGD(self, training_data, epochs, mini_batch_size, learning_rate, test_data=None):
        """
        Objective: Stochastic Gradient Descent
        1. tranining_data is a python list of tuples of the form (x, y) 
        where x is the input vector (numpy matrix of size (n x 1)) and y is the output label
        2. epochs is the number of epochs
        3. mini_batch_size is the size of single mini-batch on which we will perform the backpropogation
        4. learning_rate is learning rate!
        5. test_data is optional in the same format as training data: can be used to calculate accuracy on each
        iteration. (Ignore test_data while implementing for first time)
        """
        num_batches = len(training_data)/mini_batch_size
        #TODO: Create batches, and call apply_backprop_on_batch on each batch
        for it in range(epochs):
            ####
        
        pass
    
    def apply_backprop_on_batch(self, batch, learning_rate):
        """
        Applies the backprop on the given batch and updates weights and biases
        
        """
        gradient_biases = [np.zeros(b.shape) for b in self.biases]
        gradient_weights = [np.zeros(w.shape) for w in self.weights]
        
        for x,y in batch:
            # x, y is one training example
            gradient_biases_on_example, gradient_weights_on_example = self.apply_backprop_on_example(x,y)
            # TODO
            pass
        
    def apply_backprop_on_example(self, x, y ):
        """
        Applies backpropagation and calculates partial derivates for all the weights and biases
        for one given example (x,y)
        NOTE: It does NOT updates weights/biases. That is done by the caller function
        """
        pass
        return biases_gradients, weigths_gradients
    
    def test_accuracy(self, test_data):
        pass

"""# Digit recognition using Network

---
We import the data from mnist.
"""

from keras.datasets import mnist  # NOTE: keras is only used to import data

(x_train, y_train), (x_test, y_test) = mnist.load_data()
y_train_vectorized = list(map(vectorized_result, y_train))   # This will convert digit 0 to [1.0,0,0,0..0], 1 to [0,1.0,0,0...0] and so on...
x_train_vectorized = list(map(lambda x: np.reshape(x, (784,1))/255, x_train)) 
# print(y_train_vectorized[0])

training_data = list(zip(x_train_vectorized, y_train_vectorized))
# print(training_data[0])

x_test_vectorized = list(map(lambda x: np.reshape(x, (784,1))/255, x_test)) 
test_data = list(zip(x_test_vectorized, y_test))   # Notice carefully that we have not vectorized the y_test

net = Network([784, 16, 10])  # activation_func = relu, activation_derivative = relu_derivative)
net.SGD(training_data[:20000], 30, 10, 0.1, test_data=test_data)   # using training_data[:20000] just to make epochs faster. Consider using full training data

i = 99
plt.imshow(x_test[i],  cmap='gray')
print("Label is:", y_test[i])

print("Network's Prediction:", np.argmax(net.feedforward(x_test_vectorized[i])))

"""**Modify the following code to plot the graphs as mentioned in the assignment.**

---
The code prints three waves on a graph. Of course, you need to change it to plot training cost and
testing cost w.r.t iteration number.
"""

n1 = 10
n2 = 20
amp = 4
time = [i/1000.0 for i in range (0,1000)] # for one second
# let angular velocity be 1 radian per second
wave1 = [amp*math.sin(n1*t)  for t in time]
wave2 = [amp*math.sin(n2*t)  for t in time]
#superposition of two waves is obtained by adding their values at each instance of time
superposition = [wave1[i] + wave2[i] for i in range(len(time))]
plt.plot(time, wave1, 'b--', label = "Wave #1")
plt.plot(time,wave2, 'g',  label = "Wave #2")
plt.plot(time, superposition, 'r-',  label = "Superposition of wave #1 and wave #2")
plt.legend()
plt.xlabel("Time - t")
plt.ylabel("Dispacement - x")
plt.title("Superposition of two waves")
plt.grid()
plt.show()

"""# NN Example \#1 : Shapes"""

!pip install drawSvg

import drawSvg as draw

def drawRect(d, x, y, color):
    d.append(draw.Rectangle(x,y,9,9, fill=color,))
    d.setPixelScale(2)  # Set number of pixels per geometry unit
    # Display in Jupyter notebook
    d.rasterize()  # Display as PNG
    return d  # Display as SVG

def rgb2hex(r,g,b):
    return "#{:02x}{:02x}{:02x}".format(r,g,b)

def flattenMatrix(shape):
    out = list()
    for row in shape:
        out.extend(row)
    return out


def drawMatrix(mat, color = None):
    # values expected b/w 0 and 1
    if color == None:
        r,g,b = 255,0,0
    else:
        r,g,b = color
    m = len(mat)
    n = len(mat[0])
    d = draw.Drawing(n*10, m*10, displayInline=False)
    for row in range(0,m):
        for col in range(0,n):
            x,y= col*10, (m-1-row)*10
            active = mat[row][col]
            color = rgb2hex(round(r*active), round(g*active), round(b*active))
            drawRect(d, x, y, color)
    return d

cap = [[0.9,0.9,0.9,0.9],[0.9,0,0,0.9],[0.9,0,0,0.9],[0,0,0,0]]
L_shape = [[0.9,0,0,0],[0.9,0,0,0],[0.9,0,0,0],[0.9,0.9,0.9,0.9]]
square = [[0.9,0.9,0.9,0.9],[0.9,0,0,0.9],[0.9,0,0,0.9],[0.9,0.9,0.9,0.9]]

mat = np.random.randint(low = 0, high = 2, size = (4,4))
drawMatrix(cap).rasterize()

drawMatrix(L_shape).rasterize()

"""# Network to detect the shapes : L-Shape, Square, Cap"""

shape_net = Network([16,6,4])   # 16 is number of inputs, 4 is the number of outputs
flatten_cap = np.array(cap).reshape((16,1))
out = shape_net.feedforward(flatten_cap)
result = np.argmax(out)
print(out)

# Create dataset for training
def getNoiseMatrix(mat):
    return np.array(mat) + np.random.normal(0, .05, np.array(mat).shape)
drawMatrix(getNoiseMatrix(square).tolist()).rasterize()

training_shapes = []
training_shapes_labels = []  ## Label would be [Square, Cap, L, other] :: [1,0,0]
## Test label mapping:  Square = 0, Cap = 1, L = 2, other = 3

def createShapeData(size):
    """
    This function is used to create artificial data for training shape detector
    """
    shapes = []
    labels = []
    for i in range(size//4):
        noised_square = getNoiseMatrix(square).reshape((16,1))
        noised_cap = getNoiseMatrix(cap).reshape((16,1))
        noised_L = getNoiseMatrix(L_shape).reshape((16,1))
        other =  np.random.normal(.5, .5, (16,1))
        
        shapes.extend([noised_square, noised_cap, noised_L, other])
        labels.extend(map(convert_to_np_label, [[1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]]))
    return shapes, labels

def convert_to_np_label(lst):
    return np.array(lst).reshape(4,1)
        
training_shapes, training_labels = createShapeData(1000) 
test_shapes, test_labels = createShapeData(200) 
# need to convert test_labels to numbers
test_labels = [np.argmax(i) for i in test_labels]
# zip the data for Network
s_training_data = list(zip(training_shapes, training_labels))
s_test_data = list(zip(test_shapes, test_labels))

print(training_labels[1])
drawMatrix(training_shapes[2].reshape(4,4).tolist()).rasterize()

"""## Train the shape_net with the generated data"""

shape_net.SGD(s_training_data, 30, 10, 0.1, s_test_data)
label_map = ["Square", "Cap", "L-Shape", "Other"]

# helper function to apply the model to get the shape name
def getShape(mat):
    out = shape_net.feedforward(mat.reshape(16,1))
    result = np.argmax(out)
    print(result)
    return label_map[result]

"""## Testing the model manually"""

test_mat = np.array([
    0.9,0.88, 0.7,  0.9,
    0.9,0.22,0.1,  0.9,
    0.9,0.21,0.02, 0.9,
    0.9,0.8, 0.9,  0.9]
).reshape((4,4))

print("Predicted Shape is: ", getShape(test_mat))
drawMatrix(test_mat.tolist())

test_mat = np.array([
    0.9,0.2, 0.9,  0.0,
    0.9,0.22,0.1,  0.01,
    0.9,0.21,0.02, 0.02,
    0.9,0.8, 0.9,  0.01]
).reshape((4,4))

print("Predicted Shape is: ", getShape(test_mat))
drawMatrix(test_mat.tolist())

#@title
# we can see the weights of the neurons using following
print(shape_net.weights[0])
